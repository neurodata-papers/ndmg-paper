{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os.path as op\n",
    "import scipy as sp\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import OrderedDict\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from scipy.linalg import svd\n",
    "from scipy.linalg import norm\n",
    "\n",
    "np.random.seed(12345678)  # for reproducibility, set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets: BNU1 (114), KKI2009 (42), NKI1 (40), SWU4 (454), HNU1 (300)\n",
      "Total Subjects: 950\n"
     ]
    }
   ],
   "source": [
    "# path = '/Users/gkiar/code/ocp/ndmg-paper/data/new/'\n",
    "# dsets = ['BNU1', 'HNU1', 'KKI2009', 'MRN114', 'MRN1313', 'NKI1', 'SWU4']\n",
    "path = '/Users/gkiar/code/ocp/ndmg-paper/data/cloud/'\n",
    "# dsets = ['BNU1', 'BNU3', 'HNU1', 'Jung2015', 'KKI2009', 'MRN114', 'MRN1313', 'NKI1', 'NKIENH', 'SWU4']\n",
    "dsets = ['BNU1', 'KKI2009', 'NKI1', 'SWU4', 'HNU1']\n",
    "# dsets = ['BNU1', 'BNU3', 'HNU1', 'Jung2015', 'MRN114', 'MRN1313', 'NKI1', 'NKIENH', 'SWU4']\n",
    "\n",
    "# dsets = ['BNU1', 'KKI2009', 'HNU1', 'NKI1', 'SWU4']\n",
    "# dsets = ['BNU1', 'HNU1', 'KKI2009', 'MRN114', 'MRN1313', 'NKI24', 'SWU4']\n",
    "# dsets = ['KKI2009']\n",
    "# dsets = ['SWU4']\n",
    "\n",
    "dir_names = [path + '/' + d for d in dsets]\n",
    "\n",
    "N = 70\n",
    "\n",
    "fs = OrderedDict()\n",
    "for idx, dd in enumerate(dsets):\n",
    "    fs[dd] = [root + \"/\" + fl for root, dirs, files in os.walk(dir_names[idx])\n",
    "              for fl in files if fl.endswith(\".gpickle\")]\n",
    "\n",
    "# ps = {os.path.splitext(os.path.basename(fl))[0] : root + \"/\" + fl\n",
    "#       for root, dirs, files in os.walk(path+'phenotypes')\n",
    "#       for fl in files if fl.endswith(\".csv\") }\n",
    "\n",
    "print \"Datasets: \" + \", \".join([fkey + ' (' + str(len(fs[fkey])) + ')'\n",
    "                                for fkey in fs])\n",
    "S = sum([len(fs[key]) for key in fs])\n",
    "print \"Total Subjects: %d\" % (S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadGraphs(filenames, verb=False):\n",
    "    \"\"\"\n",
    "    Given a list of files, returns a dictionary of graphs\n",
    "\n",
    "    Required parameters:\n",
    "        filenames:\n",
    "            - List of filenames for graphs\n",
    "    Optional parameters:\n",
    "        verb:\n",
    "            - Toggles verbose output statements\n",
    "    \"\"\"\n",
    "    #  Initializes empty dictionary\n",
    "    gstruct = OrderedDict()\n",
    "    for idx, files in enumerate(filenames):\n",
    "        if verb:\n",
    "            print \"Loading: \" + files\n",
    "        #  Adds graphs to dictionary with key being filename\n",
    "        fname = os.path.basename(files)\n",
    "        gstruct[fname] = nx.read_gpickle(files)\n",
    "    return gstruct\n",
    "\n",
    "def constructGraphDict(names, fs, verb=False):\n",
    "    \"\"\"\n",
    "    Given a set of files and a directory to put things, loads graphs.\n",
    "\n",
    "    Required parameters:\n",
    "        names:\n",
    "            - List of names of the datasets\n",
    "        fs:\n",
    "            - Dictionary of lists of files in each dataset\n",
    "    Optional parameters:\n",
    "        verb:\n",
    "            - Toggles verbose output statements\n",
    "    \"\"\"\n",
    "    #  Loads graphs into memory for all datasets\n",
    "    graphs = OrderedDict()\n",
    "    for idx, name in enumerate(names):\n",
    "        if verb:\n",
    "            print \"Loading Dataset: \" + name\n",
    "        # The key for the dictionary of graphs is the dataset name\n",
    "        graphs[name] = loadGraphs(fs[name], verb=verb)\n",
    "    return graphs\n",
    "\n",
    "def rdf(dist, ids):\n",
    "    N = dist.shape[0]\n",
    "    assert(N == len(ids))\n",
    "    uniqids = list(set(ids))\n",
    "    countvec = [ids.count(uniqid) for uniqid in uniqids]\n",
    "    scans = np.max(countvec)\n",
    "#     rdf = np.empty((N*(scans-1)))\n",
    "    rdf = []\n",
    "\n",
    "    for i in np.arange(0, N):\n",
    "        ind = [idx for idx, x in enumerate(ids) if x == ids[i]]\n",
    "        for j in ind:\n",
    "            if i != j:\n",
    "                di = deepcopy(dist[i,:])\n",
    "                di[ind] = np.inf\n",
    "                d = dist[i,j]\n",
    "                diff = di[np.where(~np.isinf(di))]\n",
    "#                 import pdb; pdb.set_trace()\n",
    "                rdf += [1.0 - ((np.sum(diff < d) + 0.5*np.sum(diff == d)) / (1.0*(N-len(ind))))]\n",
    "    return rdf\n",
    "\n",
    "def partial_disc(D, labels, subject, trial1, trial2):\n",
    "    enum = np.arange(D.shape[0])\n",
    "    idx1 = [i for i, x in enumerate(labels) if x == subject]\n",
    "    t1 = enum[idx1][trial1]\n",
    "    t2 = enum[idx1][trial2]\n",
    "    d_t1_t2 = D[t1][t2]\n",
    "    \n",
    "    idx2 = [i for i, x in enumerate(labels) if x != subject]\n",
    "    d_ra = [D[t1][x] for x in enum[idx2]]\n",
    "    \n",
    "    return np.mean(d_t1_t2 < d_ra)\n",
    "\n",
    "def distance_matrix(data, metric, symmetric = True):\n",
    "    n = data.shape[2]\n",
    "    dist_matrix = np.zeros((n, n))\n",
    "    if symmetric:\n",
    "        for i in range(n):\n",
    "            for j in range(i):\n",
    "                tmp = metric(data[:,:,i] - data[:,:,j])\n",
    "                dist_matrix[i][j] = tmp\n",
    "                dist_matrix[j][i] = tmp\n",
    "    else:\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                dist_matrix[i][j] = metric(data[i] - data[j])\n",
    "    return dist_matrix\n",
    "\n",
    "def discriminibility(data, labels, metric):\n",
    "    dist_matrix = distance_matrix(data, metric)\n",
    "    partials = []\n",
    "    for s in list(set(labels)):\n",
    "        num = ids.count(s)\n",
    "        for t in range(num):\n",
    "            for tt in range(num):\n",
    "                if tt != t:\n",
    "                    p = partial_disc(dist_matrix, labels, s, t, tt)\n",
    "                    partials.append(p)\n",
    "    return dist_matrix, np.mean(partials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graphs = constructGraphDict(dsets, fs, verb=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All subjs, ID = subj id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (70, 70, 114), 114\n",
      "BNU1 (114): 0.984257518797\n",
      "Shape: (70, 70, 42), 42\n",
      "KKI2009 (42): 1.0\n",
      "Shape: (70, 70, 40), 40\n",
      "NKI1 (40): 0.983552631579\n",
      "Shape: (70, 70, 454), 454\n",
      "SWU4 (454): 0.883630267826\n",
      "Shape: (70, 70, 300), 300\n",
      "HNU1 (300): 0.993232439336\n",
      "Shape: (70, 70, 950), 950\n",
      "Total (950): 0.979084429723\n"
     ]
    }
   ],
   "source": [
    "bigmat = np.zeros((70, 70, 0))\n",
    "bigids = []\n",
    "\n",
    "for dset in graphs.keys():\n",
    "    mat = np.zeros((N, N, len(graphs[dset].keys())))\n",
    "    ids = []\n",
    "    c = 0\n",
    "    for subj in graphs[dset].keys():\n",
    "        ids += [subj.split(\"_\")[0].split(\"-\")[1]]\n",
    "        \n",
    "        tmpg = np.array(nx.adj_matrix(graphs[dset][subj]).todense())\n",
    "        mat[:, :, c] = tmpg\n",
    "        c += 1\n",
    "    print \"Shape: {}, {}\".format(mat.shape, len(ids))\n",
    "    \n",
    "    dist = distance_matrix(mat, norm)\n",
    "    myrdf = rdf(dist, ids)\n",
    "    disc = np.mean(myrdf)\n",
    "#     dist, disc = discriminibility(mat, ids, norm)\n",
    "    print '{} ({}): {}'.format(dset, len(graphs[dset].keys()), disc)\n",
    "\n",
    "    f = open('ids_{}.csv'.format(dset), 'wb')\n",
    "    f.write(\"\\n\".join([i for i in ids]))\n",
    "    f.close()\n",
    "    np.savetxt('dist_{}.csv'.format(dset), dist, delimiter=\",\")\n",
    "    \n",
    "    bigmat = np.append(bigmat, mat, axis=2)\n",
    "    bigids += ids\n",
    "\n",
    "print \"Shape: {}, {}\".format(bigmat.shape, len(bigids))\n",
    "bigdist = distance_matrix(bigmat, norm)\n",
    "myrdf = rdf(bigdist, bigids)\n",
    "disc = np.mean(myrdf)\n",
    "print 'Total ({}): {}'.format(S, disc)\n",
    "\n",
    "f = open('ids_all.csv', 'wb')\n",
    "f.write(\"\\n\".join([i for i in bigids]))\n",
    "f.close()\n",
    "np.savetxt('dist_all.csv', dist, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
